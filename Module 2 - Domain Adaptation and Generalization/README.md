# Domain Adaptation and Generalization

This assignment examines the challenges of Domain Adaptation (DA) and Domain Generalization (DG) in machine learning, focusing on improving model robustness under distribution shifts. It begins with fundamental techniques in unsupervised DA, where the objective is to reduce the divergence between a labeled source domain and an unlabeled target domain. The assignment highlights the trade-off between aligning source and target feature distributions and preserving model discriminativeness. By engaging with DA, students develop an understanding of how to adapt models to new environments without access to labeled target data.

The assignment then progresses to Domain Generalization, where models are trained across multiple source domains with the aim of generalizing to unseen target domains. It introduces invariant learning methods such as Invariant Risk Minimization (IRM), along with optimization techniques including sharpness-aware minimization and Group Distributionally Robust Optimization (Group DRO) for worst-case performance. The goal is to equip students with practical experience and theoretical insight into both adapting models to new distributions and designing models that can generalize beyond observed domains. Through targeted experimentation, the assignment encourages critical evaluation of the trade-offs involved in domain alignment and robustness-focused training.
