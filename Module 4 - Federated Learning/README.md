# Federated Learning

This assignment investigates Federated Learning (FL), a decentralized training paradigm where multiple clients collaboratively optimize a shared global model without exchanging local data. Instead of centralizing datasets, each client maintains its own data and only shares model updates with a central server. The objective is to minimize a weighted global loss, ensuring that each client's contribution reflects its dataset size. Training occurs over multiple communication rounds, during which the global model is broadcast to selected clients, locally optimized, and then aggregated by the server. This process allows learning from distributed data while preserving privacy. However, it introduces challenges absent in centralized training, particularly under statistically heterogeneous (non-IID) data distributions.

The assignment first examines fundamental approaches such as Federated SGD (FedSGD) and Federated Averaging (FedAvg), highlighting their strengths and limitations. While FedSGD theoretically aligns with centralized SGD, it is highly communication-intensive. FedAvg alleviates this by allowing multiple local updates before synchronization but suffers from client drift when data distributions vary significantly across clients. The module then explores more advanced federated optimization strategies designed to mitigate statistical heterogeneity, including FedProx (which introduces a proximal regularizer to prevent excessive local divergence), FedDyn (which dynamically adjusts local objectives to align with the global optimum), SCAFFOLD (which uses control variates to correct drift), and FedGH (which resolves conflicting client gradients during aggregation). Additionally, the assignment includes FedSAM, which incorporates sharpness-aware minimization into local updates to improve generalization and reduce overfitting to client-specific data. By implementing and analyzing these methods, the assignment aims to develop an understanding of optimization under distribution heterogeneity in FL and guide students in evaluating trade-offs among stability, communication efficiency, and convergence performance.
