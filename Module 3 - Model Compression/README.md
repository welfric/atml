# Model Compression Techniques

This assignment explores the core principles and practical applications of model compression, motivated by the increasing use of large neural networks in environments with limited computational resources. As architectures such as ResNet-50 demand significant memory and processing capacity, reducing model size while preserving performance becomes critical, particularly for real-time and resource-constrained settings such as mobile devices, embedded systems, and wearable technology. Efficient deep learning approaches not only enable deployment on devices with limited memory, energy, and processing power but also support broader applications in distributed systems, edge computing, and FPGA-based AI platforms. The assignment aims to demonstrate how substantial reductions in model parameters and computational overhead can be achieved without severely impacting accuracy.

The assignment investigates three primary model compression techniques: pruning, quantization, and knowledge distillation. Pruning focuses on selectively removing redundant or low-importance connections, either at the weight level (unstructured) or by eliminating entire channels or neurons (structured), analyzing its effect on architecture suitability and decision-making. Quantization reduces the precision of weights and activations to lower bit-width formats to improve memory efficiency and inference speed, with emphasis on comparing post-training quantization and quantization-aware training. Finally, knowledge distillation explores transferring knowledge from a larger teacher model to a smaller student, assessing strategies such as logit matching, feature matching, and contrastive representation distillation. The assignment also considers whether teacher size correlates with student performance and examines effects on robustness and out-of-distribution generalization. The goal is to provide hands-on insight into compression mechanisms, their trade-offs, and their practical relevance to real-world deployment.
