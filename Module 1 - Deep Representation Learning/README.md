# Deep Representation Learning

This first module of Deep Representation Learning examines how different vision model architectures encode and interpret visual information, with a specific focus on their inductive biases. Inductive bias refers to the inherent assumptions built into a model that shape its learning behavior and generalization. Architectures such as Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), generative models including Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs), and contrastive models like CLIP are designed with distinct structural principles that influence their preference for certain types of visual cues. For instance, CNNs emphasize local texture patterns due to the nature of convolutional filters, whereas ViTs use global self-attention, allowing them to focus more on broader structural and semantic features. These architectural differences result in varying levels of robustness when models encounter data that deviates from training conditions.

The module is structured around hypothesis-driven experimentation that prioritizes interpretation and reasoning over exhaustive training or tuning. Students will explore how discriminative, generative, and contrastive models respond to both standard inputs and perturbed or out-of-distribution examples. Experiments will assess how models manage challenges such as texture–shape ambiguity, spatial translation, patch permutation, and style-based distortions. Comparisons between CNNs and ViTs will highlight differences in shape versus texture bias, while experiments on VAEs and GANs will illustrate trade-offs between smoothness of latent space, diversity, and fidelity. Additionally, the module investigates whether CLIP’s multimodal training promotes more human-like inductive biases and enables stronger zero-shot generalization. Ultimately, the module aims to evaluate how architectural and semantic preferences influence model behavior under distribution shifts, encouraging critical analysis of the relationship between inductive bias and out-of-distribution performance.
